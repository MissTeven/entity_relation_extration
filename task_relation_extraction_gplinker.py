#! -*- coding:utf-8 -*-# 三元组抽取任务，基于GlobalPointer的仿TPLinker设计# 文章介绍：https://kexue.fm/archives/8888# 数据集：http://ai.baidu.com/broad/download?dataset=sked# 最优f1=0.827# 说明：由于使用了EMA，需要跑足够多的步数(5000步以上）才生效，如果#      你的数据总量比较少，那么请务必跑足够多的epoch数，或者去掉EMA。import jsonimport numpy as npfrom bert4keras.backend import keras, Kfrom bert4keras.backend import sparse_multilabel_categorical_crossentropyfrom bert4keras.tokenizers import Tokenizer# from bert4keras.layers import EfficientGlobalPointer as GlobalPointerfrom bert4keras.layers import GlobalPointerfrom bert4keras.models import build_transformer_modelfrom bert4keras.optimizers import Adam, extend_with_exponential_moving_averagefrom bert4keras.snippets import sequence_padding, DataGeneratorfrom bert4keras.snippets import open, to_arrayfrom tqdm import tqdm# from sklearn.model_selection import KFoldmaxlen = 512batch_size = 64# config_path = '/Users/edz/Downloads/chinese_pretrain_mrc_macbert_large_tf/config.json'# checkpoint_path = '/Users/edz/Downloads/chinese_pretrain_mrc_macbert_large_tf/chinese_pretrain_mrc_macbert_large.ckpt'# dict_path = '/Users/edz/Downloads/chinese_pretrain_mrc_macbert_large_tf/vocab.txt'config_path = '/Users/edz/Downloads/NEZHA-Large-WWM/bert_config.json'checkpoint_path = '/Users/edz/Downloads/NEZHA-Large-WWM/model.ckpt-346400'dict_path = '/Users/edz/Downloads/NEZHA-Large-WWM/vocab.txt'def load_data(filename):    """加载数据    单条格式：{'text': text, 'spo_list': [(s, p, o)]}    """    D = []    with open(filename, encoding='utf-8') as f:        for l in f:            l = json.loads(l)            D.append({                'text': l['text'],                'spo_list': [(spo['h']['name'], spo['relation'], spo['t']['name'])                             for spo in l['spo_list']]            })    return D# 加载数据集train_data = load_data('/Users/edz/Downloads/达观知识图谱数据/train.json')valid_data = load_data('/Users/edz/Downloads/达观知识图谱数据/train.json')[:100]predicate2id, id2predicate = {}, {}with open('/Users/edz/Downloads/达观知识图谱数据/train.json') as f:    for l in f:        l = json.loads(l)        for s in l['spo_list']:            if s['relation'] not in predicate2id:                id2predicate[len(predicate2id)] = s['relation']                predicate2id[s['relation']] = len(predicate2id)# 建立分词器tokenizer = Tokenizer(dict_path, do_lower_case=True)def search(pattern, sequence):    """从sequence中寻找子串pattern    如果找到，返回第一个下标；否则返回-1。    """    n = len(pattern)    for i in range(len(sequence)):        if sequence[i:i + n] == pattern:            return i    return -1class data_generator(DataGenerator):    """数据生成器    """    def __iter__(self, random=False):        batch_token_ids, batch_segment_ids = [], []        batch_entity_labels, batch_head_labels, batch_tail_labels, batch_head_entity_labels, batch_tail_entity_labels = \            [], [], [], [], []        for is_end, d in self.sample(random):            token_ids, segment_ids = tokenizer.encode(d['text'], maxlen=maxlen)            # 整理三元组 {(s, o, p)}            spoes = set()            for s, p, o in d['spo_list']:                s = tokenizer.encode(s)[0][1:-1]                p = predicate2id[p]                o = tokenizer.encode(o)[0][1:-1]                sh = search(s, token_ids)                oh = search(o, token_ids)                if sh != -1 and oh != -1:                    spoes.add((sh, sh + len(s) - 1, p, oh, oh + len(o) - 1))            # 构建标签            entity_labels = [set() for _ in range(2)]            head_entity_labels = [set() for _ in range(3)]            tail_entity_labels = [set() for _ in range(3)]            head_labels = [set() for _ in range(len(predicate2id))]            tail_labels = [set() for _ in range(len(predicate2id))]            for sh, st, p, oh, ot in spoes:                entity_labels[0].add((sh, st))                entity_labels[1].add((oh, ot))                head_entity_labels[p_head_map.get(p)].add((sh, st))                tail_entity_labels[p_tail_map.get(p)].add((oh, ot))                head_labels[p].add((sh, oh))                tail_labels[p].add((st, ot))            for label in entity_labels + head_labels + tail_labels + head_entity_labels + tail_entity_labels:                if not label:  # 至少要有一个标签                    label.add((0, 0))  # 如果没有则用0填充            entity_labels = sequence_padding([list(l) for l in entity_labels])            head_entity_labels = sequence_padding([list(l) for l in head_entity_labels])            tail_entity_labels = sequence_padding([list(l) for l in tail_entity_labels])            head_labels = sequence_padding([list(l) for l in head_labels])            tail_labels = sequence_padding([list(l) for l in tail_labels])            # 构建batch            batch_token_ids.append(token_ids)            batch_segment_ids.append(segment_ids)            batch_entity_labels.append(entity_labels)            batch_head_entity_labels.append(head_entity_labels)            batch_tail_entity_labels.append(tail_entity_labels)            batch_head_labels.append(head_labels)            batch_tail_labels.append(tail_labels)            if len(batch_token_ids) == self.batch_size or is_end:                batch_token_ids = sequence_padding(batch_token_ids)                batch_segment_ids = sequence_padding(batch_segment_ids)                batch_entity_labels = sequence_padding(                    batch_entity_labels, seq_dims=2                )                batch_head_entity_labels = sequence_padding(                    batch_head_entity_labels, seq_dims=2                )                batch_tail_entity_labels = sequence_padding(                    batch_tail_entity_labels, seq_dims=2                )                batch_head_labels = sequence_padding(                    batch_head_labels, seq_dims=2                )                batch_tail_labels = sequence_padding(                    batch_tail_labels, seq_dims=2                )                yield [batch_token_ids, batch_segment_ids], [                    batch_entity_labels, batch_head_labels, batch_tail_labels, batch_head_entity_labels,                    batch_tail_entity_labels                ]                batch_token_ids, batch_segment_ids = [], []                batch_entity_labels, batch_head_labels, batch_tail_labels, batch_head_entity_labels, \                batch_tail_entity_labels = [], [], [], [], []def globalpointer_crossentropy(y_true, y_pred):    """给GlobalPointer设计的交叉熵    """    shape = K.shape(y_pred)    y_true = y_true[..., 0] * K.cast(shape[2], K.floatx()) + y_true[..., 1]    y_pred = K.reshape(y_pred, (shape[0], -1, K.prod(shape[2:])))    loss = sparse_multilabel_categorical_crossentropy(y_true, y_pred, True)    return K.mean(K.sum(loss, axis=1))# 加载预训练模型base = build_transformer_model(    config_path=config_path,    model='nezha',    checkpoint_path=checkpoint_path,    return_keras_model=False)print(base.layers)base_model_output = keras.layers.average([base.layers["Transformer-23-FeedForward-Norm"].output,                                          base.layers["Transformer-22-FeedForward-Norm"].output,                                          base.layers["Transformer-21-FeedForward-Norm"].output,                                          base.layers["Transformer-20-FeedForward-Norm"].output])base_model_output = keras.layers.Dropout(0.3)(base_model_output)# 预测结果entity_output = GlobalPointer(heads=2, head_size=64)(base_model_output)head_entity_output = GlobalPointer(heads=3, head_size=64)(base_model_output)tail_entity_output = GlobalPointer(heads=3, head_size=64)(base_model_output)head_output = GlobalPointer(    heads=len(predicate2id), head_size=64, RoPE=False, tril_mask=False)(base_model_output)tail_output = GlobalPointer(    heads=len(predicate2id), head_size=64, RoPE=False, tril_mask=False)(base_model_output)outputs = [entity_output, head_output, tail_output, head_entity_output, tail_entity_output]# 构建模型import tensorflow as tfopt = tf.keras.optimizers.Adam(1e-5, clipvalue=1)#add a line  混合精度训练opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(            opt,            loss_scale='dynamic')AdamEMA = extend_with_exponential_moving_average(Adam, name='AdamEMA')optimizer = AdamEMA(learning_rate=1e-5)model = keras.models.Model(base.model.inputs, outputs)model.compile(loss=globalpointer_crossentropy, optimizer=optimizer)model.summary()def extract_spoes(text, threshold=0):    """抽取输入text所包含的三元组    """    tokens = tokenizer.tokenize(text, maxlen=maxlen)    mapping = tokenizer.rematch(text, tokens)    token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)    token_ids, segment_ids = to_array([token_ids], [segment_ids])    outputs = model.predict([token_ids, segment_ids])    outputs = [o[0] for o in outputs]    # 抽取subject和object    subjects, objects = set(), set()    outputs[0][:, [0, -1]] -= np.inf    outputs[0][:, :, [0, -1]] -= np.inf    for l, h, t in zip(*np.where(outputs[0] > threshold)):        if l == 0:            subjects.add((h, t))        else:            objects.add((h, t))    # 识别对应的predicate    spoes = set()    for sh, st in subjects:        for oh, ot in objects:            p1s = np.where(outputs[1][:, sh, oh] > threshold)[0]            p2s = np.where(outputs[2][:, st, ot] > threshold)[0]            ps = set(p1s) & set(p2s)            for p in ps:                spoes.add((                    text[mapping[sh][0]:mapping[st][-1] + 1], id2predicate[p],                    text[mapping[oh][0]:mapping[ot][-1] + 1]                ))    return list(spoes)def predict_spoes(text, threshold=0):    """抽取输入text所包含的三元组    """    tokens = tokenizer.tokenize(text, maxlen=maxlen)    mapping = tokenizer.rematch(text, tokens)    token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)    token_ids, segment_ids = to_array([token_ids], [segment_ids])    outputs = model.predict([token_ids, segment_ids])    outputs = [o[0] for o in outputs]    # 抽取subject和object    subjects, objects = set(), set()    outputs[0][:, [0, -1]] -= np.inf    outputs[0][:, :, [0, -1]] -= np.inf    for l, h, t in zip(*np.where(outputs[0] > threshold)):        if l == 0:            subjects.add((h, t))        else:            objects.add((h, t))    # 识别对应的predicate    spoes = {}    for sh, st in subjects:        for oh, ot in objects:            p1s = np.where(outputs[1][:, sh, oh] > threshold)[0]            p2s = np.where(outputs[2][:, st, ot] > threshold)[0]            ps = set(p1s) & set(p2s)            for p in ps:                psh = mapping[sh][0]                pst = mapping[st][-1] + 1                poh = mapping[oh][0]                pot = mapping[ot][-1] + 1                h = text[psh:pst]                t = text[poh:pot]                p = id2predicate[p]                key = '_'.join([h, p, t])                if not spoes.get(key):                    p_spo = (                        [h, psh, pst], p, [t, poh, pot]                    )                    spoes[key] = p_spo    spo_list = []    # {"h": {"name": "转向轴处", "pos": [23, 27]}, "t": {"name": "缺油", "pos": [27, 29]}, "relation": "部件故障"}    for key, value in spoes.items():        spo_list.append({"h": {"name": value[0][0], "pos": [value[0][1], value[0][2]]},                         "t": {"name": value[2][0], "pos": [value[2][1], value[2][2]]},                         "relation": value[1]})    return spo_listclass SPO(tuple):    """用来存三元组的类    表现跟tuple基本一致，只是重写了 __hash__ 和 __eq__ 方法，    使得在判断两个三元组是否等价时容错性更好。    """    def __init__(self, spo):        self.spox = (            tuple(tokenizer.tokenize(spo[0])),            spo[1],            tuple(tokenizer.tokenize(spo[2])),        )    def __hash__(self):        return self.spox.__hash__()    def __eq__(self, spo):        return self.spox == spo.spoxdef evaluate(data):    """评估函数，计算f1、precision、recall    """    X, Y, Z = 1e-10, 1e-10, 1e-10    f = open('dev_pred.json', 'w', encoding='utf-8')    pbar = tqdm()    for d in data:        R = set([SPO(spo) for spo in extract_spoes(d['text'])])        T = set([SPO(spo) for spo in d['spo_list']])        X += len(R & T)        Y += len(R)        Z += len(T)        f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z        pbar.update()        pbar.set_description(            'f1: %.5f, precision: %.5f, recall: %.5f' % (f1, precision, recall)        )        s = json.dumps({            'text': d['text'],            'spo_list': list(T),            'spo_list_pred': list(R),            'new': list(R - T),            'lack': list(T - R),        },                       ensure_ascii=False,                       indent=4)        f.write(s + '\n')    pbar.close()    f.close()    return f1, precision, recallclass Evaluator(keras.callbacks.Callback):    """评估与保存    """    def __init__(self):        self.best_val_f1 = 0.    def on_epoch_end(self, epoch, logs=None):        # optimizer.apply_ema_weights()        f1, precision, recall = evaluate(valid_data)        if f1 >= self.best_val_f1:            self.best_val_f1 = f1            model.save_weights('best_model.weights')        # optimizer.reset_old_weights()        print(            'f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\n' %            (f1, precision, recall, self.best_val_f1)        )if __name__ == '__main__':    # num_splits = 5    # kf = KFold(n_splits=num_splits, shuffle=True, random_state=2022)    # fold = 0    # for fold, (train_index, val_index) in enumerate(kf.split(train_data)):    #     fold += 1    #     print("=" * 80)    #     print(f"正在训练第 {fold} 折的数据")    #     # 划分训练集和验证集    #     train_data = [train_data[i] for i in train_index]    #     valid_data = [train_data[i] for i in val_index]    #     model_savepath = f'../best_model/best_model_fold{fold}.weights'    #     train_generator = data_generator(train_data, batch_size)    #     evaluator = Evaluator()    #    #     model.fit(    #         train_generator.forfit(),    #         steps_per_epoch=len(train_generator),    #         epochs=200,    #         callbacks=[evaluator]    #     )    train_generator = data_generator(train_data, batch_size)    evaluator = Evaluator()    model.fit(        train_generator.forfit(),        steps_per_epoch=len(train_generator),        epochs=20,        callbacks=[evaluator]    )    # import json    # model.load_weights('/Users/edz/Downloads/best_model-roberta-1008.weights')    #    # papers = []    # for line in open("/Users/edz/Downloads/达观知识图谱数据/evalA.json", 'r', encoding='utf-8'):    #     papers.append(json.loads(line))    #    # count = 0    # with open('evalResult_1009.json', 'a+', encoding='utf8') as f:    #     for i in papers:    #         spo_list = predict_spoes(i['text'])    #         print(spo_list)    #         json.dump({"ID": i["ID"], "text": i["text"], "spo_list": spo_list}, f, ensure_ascii=False)    #         f.write('\r\n')    #         count += 1else:    model.load_weights('best_model.weights')