#! -*- coding:utf-8 -*-# 三元组抽取任务，基于GlobalPointer的仿TPLinker设计# 文章介绍：https://kexue.fm/archives/8888# 数据集：http://ai.baidu.com/broad/download?dataset=sked# 最优f1=0.827# 说明：由于使用了EMA，需要跑足够多的步数(5000步以上）才生效，如果#      你的数据总量比较少，那么请务必跑足够多的epoch数，或者去掉EMA。import jsonfrom bert4keras.backend import keras, Kfrom bert4keras.backend import sparse_multilabel_categorical_crossentropyfrom bert4keras.tokenizers import Tokenizerfrom bert4keras.optimizers import Adam, extend_with_exponential_moving_averagefrom bert4keras.snippets import open, to_arrayfrom model import GlobalPointerDataGeneratorfrom model import GlobalPointerWrapfrom model import GlobalPointerEvaluate# 构建模型import tensorflow as tf# from sklearn.model_selection import KFoldmaxlen = 512batch_size = 64config_path = '/Users/edz/Downloads/NEZHA-Large-WWM/bert_config.json'checkpoint_path = '/Users/edz/Downloads/NEZHA-Large-WWM/model.ckpt-346400'dict_path = '/Users/edz/Downloads/NEZHA-Large-WWM/vocab.txt'def load_data(filename):    """加载数据    单条格式：{'text': text, 'spo_list': [(s, p, o)]}    """    D = []    with open(filename, encoding='utf-8') as f:        for l in f:            l = json.loads(l)            D.append({                'text': l['text'],                'spo_list': [(spo['h']['name'], spo['relation'], spo['t']['name'])                             for spo in l['spo_list']]            })    return Ddef globalpointer_crossentropy(y_true, y_pred):    """给GlobalPointer设计的交叉熵    """    shape = K.shape(y_pred)    y_true = y_true[..., 0] * K.cast(shape[2], K.floatx()) + y_true[..., 1]    y_pred = K.reshape(y_pred, (shape[0], -1, K.prod(shape[2:])))    loss = sparse_multilabel_categorical_crossentropy(y_true, y_pred, True)    return K.mean(K.sum(loss, axis=1))def train():    # 加载数据集    train_data = load_data('/Users/edz/Downloads/达观知识图谱数据/train.json')    valid_data = load_data('/Users/edz/Downloads/达观知识图谱数据/train.json')[:100]    predicate2id, id2predicate = {}, {}    with open('/Users/edz/Downloads/达观知识图谱数据/train.json') as f:        for l in f:            l = json.loads(l)            for s in l['spo_list']:                if s['relation'] not in predicate2id:                    id2predicate[len(predicate2id)] = s['relation']                    predicate2id[s['relation']] = len(predicate2id)    opt = tf.keras.optimizers.Adam(1e-5, clipvalue=1)    # add a line  混合精度训练    opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(        opt,        loss_scale='dynamic')    AdamEMA = extend_with_exponential_moving_average(Adam, name='AdamEMA')    optimizer = AdamEMA(learning_rate=1e-5)    model = GlobalPointerWrap(config_path, checkpoint_path, predicate2id)    model.compile(loss=globalpointer_crossentropy, optimizer=optimizer)    model.summary()    # 建立分词器    tokenizer = Tokenizer(dict_path, do_lower_case=True)    train_generator = GlobalPointerDataGenerator(train_data, batch_size, tokenizer=tokenizer, max_token_length=maxlen,                                                 predicate2id=predicate2id)    evaluator = GlobalPointerEvaluate(tokenizer=tokenizer, model=model, valid_data=valid_data, max_token_length=maxlen,                                      model_weight_file_path='best_model.weights')    model.fit(        train_generator.forfit(),        steps_per_epoch=len(train_generator),        epochs=20,        callbacks=[evaluator]    )if __name__ == '__main__':    train()